stage_name: "contrastive_pretraining"
base_model: "vinai/phobert-base"
base_model_cache_dir: "embeddings/base_models"
output_dir: "./checkpoints/stage1"

data:
  train_path: "data/processed/stage1_pseudo_sents/train_data.json"
  valid_path: "data/processed/stage1_pseudo_sents/valid_data.json"
  num_workers: 16

model:
  span_method: "attentive"  
  cls_method: "last"  
  projection_dim: 256
  temperature: 0.3
  freeze_base: False
  fusion_hidden_dim: 512
  layerwise_attn_dim: 128
  dropout: 0.1

training:
  ckpt_interval: 10
  shuffle: False
  batch_size: 128
  epochs: 20
  learning_rate: 2e-5
  warmup_steps: 1000
  max_seq_length: 128
  save_steps: 1000
  optimizer:
    name: "adamw"
    lr_base: 2e-5        
    lr_custom: 5e-4 
    weight_decay: 0.01
    eps: 1e-8
    betas: [0.9, 0.999]
